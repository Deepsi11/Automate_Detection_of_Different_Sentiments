# -*- coding: utf-8 -*-
"""Automate_Sentiment_Analysis_Using_ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gu3k-kfpNyBL8XHFKJnwY9qBURv7iV5s

#**AUTOMATE DETECTION OF DIFFERENT SENTIMENTS FROM PARAGRAPHS USING RULE BASED LEARNING**

#**IMPORTING THE REQUIRED DEPENDENCIES**
"""

# Importing the libraries
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
from collections import Counter
import nltk
import string
from nltk.tokenize import word_tokenize
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.corpus import wordnet as wn
from nltk import wordpunct_tokenize
from nltk import WordNetLemmatizer
from nltk import sent_tokenize
from nltk import pos_tag
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

"""#**LOADING THE DATASET**"""

# MOUNTING THE GOOGLE DRIVE
from google.colab import drive
drive.mount('/content/drive')

# Loading the Dataset
data = pd.read_csv('/content/drive/MyDrive/AUTOMATE_SENTIMENT/IMDB Dataset.csv')

"""#**ANALYSING THE DATASET**"""

# Printing the first five points from the data
data.head()

# Printing the unique sentiments present in the data
data["sentiment"].unique()

# Printing the firts review along with its sentiment and lenght
print(data["review"][0])
print()
print(data["sentiment"][0])
len(data["review"][0])

"""#**DATA VISUALIZATION**"""

# CREATINNG SOME VISUALIZATION
import pandas as pd
from textblob import TextBlob
import seaborn as sns
import matplotlib.pyplot as plt

# Function to get sentiment
def get_sentiment(text):
    blob = TextBlob(text)
    return blob.sentiment.polarity

# Apply the function to the reviews
data['sentiment'] = data['review'].apply(get_sentiment)

# Categorize sentiments
def categorize_sentiment(polarity):
    if polarity > 0:
        return 'Positive'
    elif polarity < 0:
        return 'Negative'
    else:
        return 'Neutral'

data['sentiment_category'] = data['sentiment'].apply(categorize_sentiment)

# Plotting the bar chart using Seaborn
plt.figure(figsize=(5, 5))
sns.countplot(x='sentiment_category', data=data, palette={'Positive': 'green', 'Negative': 'red', 'Neutral': 'blue'})
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.title('Sentiment Distribution in Reviews')
plt.show()

"""#**TEXT PRE-PROCESSING**"""

# CONVERTING TEXT TO LOWERCASE
from string import punctuation
data['review'] = data['review'].str.lower()

# CREATING TRANSLATION TABLE TO REMOVE ALL THE PUNCTUATIONS
table = str.maketrans('', '', string.punctuation)
for i in range(len(data['review'])):
    data["review"][i] = data["review"][i].replace("<br />","").translate(table)

# LET'S HAVE A LOOK AT IT
data["review"][0]

# CONVERTING NUMBER TO STRINGS
# INFLECT IS A LIBRARY TO CONVERT NUMBERS INTO ENGLISH WORDS
import inflect
p = inflect.engine()
for i in range(len(data['review'])):
    data["review"][i] =' '.join([str(p.number_to_words(((x)))) if x.isnumeric() else x for x in data["review"][i].split()])

# REMOVING STOPWORDS
stop_words = set(stopwords.words('english'))
print(stop_words)

# CHECKING THE DISTRIBUTION OF REVIEW LENGTHS
review_lens = Counter([len(x) for x in data["review"].values])
print("Zero-length reviews: {}".format(review_lens[0]))
print("Maximum review length: {}".format(max(review_lens)))
print('Number of reviews before removing outliers: ', len(data['review']))
zero_idx = [ii for ii, review in enumerate(data.review) if len(review)==0]
print(zero_idx,"index of review with 0 length")

"""#**TOKENIZING**"""

# TOKENIZING THE WORDS
word_tokens = word_tokenize(data["review"][0])
filteres_review= [w for w in word_tokens if not w in stop_words]
print(filteres_review)

"""#**FREQUNECY DISTRIBUTION**"""

# COMPUTING THE FREQUNECY DISTRIBUTION OF TOKENS
from nltk.probability import FreqDist
fdist = FreqDist(filteres_review)
print('Sampling ', fdist)
print('The first 3 frequently used tokens are')
fdist.most_common(10)

"""#**LABEL ENCODING**"""

# ENCODING THE LABELS
encoded = {"sentiment":{"positive": 1, "negative": 0}}

# REPLACING WITH ENCODED LABELS
data.replace(encoded, inplace=True)

# LOOKING AT FIRST FIVE SENTIMENT POINTS
data["sentiment"].head()

"""#**TFIDF VECTORIZER**"""

# TRANSFORMING THE TEXT USING TFIDF VECTORIZER
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(sublinear_tf=True, min_df=2000, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')
features = tfidf.fit_transform(data.review).toarray()
labels = data.sentiment
features.shape

"""#**TRAIN-TEST SPLIT**"""

# SPLITTING THE DATASET INTO TRAIN AND TEST DATA
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data["review"], data['sentiment'],test_size=0.3, random_state=42)

"""#**COUNT VECTORIZER**"""

# USING COUNT VECTORIZER TO CONVERT NUMERICAL DATA INTO TEXT
from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer()
X_train_counts = count_vect.fit_transform(X_train)

"""#**TFIDF TRANSFORMER**"""

# CONVERTING TEXT TO NUMERICAL REPRESENTATION
from sklearn.feature_extraction.text import TfidfTransformer
tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)

"""#**MODEL BUILDING**

#**MULTINOMIAL NAIVE BAYES**
"""

from sklearn.naive_bayes import MultinomialNB
clf = MultinomialNB().fit(X_train_tfidf, y_train)

"""#**EVALUATING/PREDICTIONS**"""

# MAKING PREDICTIONS
y_=clf.predict(count_vect.transform(X_test))

from sklearn.metrics import accuracy_score
print("test accuracy")
test_accuracy =accuracy_score(y_test, y_,normalize=True)
print(test_accuracy)

"""##**Achived an accuracy of --------- 83 %**"""